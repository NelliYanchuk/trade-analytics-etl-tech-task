SQL
Задание: Построение ETL-процесса по обработке сделок клиентов
Цель:
Реализовать ETL-пайплайн, обрабатывающий торговые данные из CSV-файла и
записывающий агрегированные результаты в базу данных, а также
продемонстрировать настройку CI/CD и готовность к масштабированию.
Требования:
1. Исходные данные:
○ CSV-файл trades.csv со следующими полями:
timestamp, user_id, client_type, symbol, side, quantity, price
2. ETL:
○ Разделите процесс на модули: extract, transform, load.
○ На этапе трансформации:
■ Преобразуйте timestamp в week_start_date (понедельник)
■ Агрегируйте данные по:
■ week_start_date
■ client_type (gold, silver, bronze)
■ user_id
■ symbol
■ Рассчитайте:
■ total_volume
■ total_pnl (опционально)
■ trade_count
■ Загрузите результат:
■ База: agg_result.db
■ Таблица: agg_trades_weekly
○ Настройте GitHub Actions:
■ Пайплайн запускается при push или workflow_dispatch
■ Автоматически выполняется ETL
○ Базу можно сохранить в agg_result.db
3. Отчётность:
○ Постройте 1–2 графика по агрегированным данным (по желанию)
○ В конце получите и сохраните:
■ CSV/Excel-файл с топ-3 bronze-клиентами с наибольшим:
■ total_volume
■ total_pnl (опционально)
■ Результаты положите в output/top_clients.xlsx или .csv
4. README.md:
○ Опишите:
■ Как запустить ETL вручную
■ Как работает CI/CD
■ Как бы вы адаптировали решение под 100+ млн строк:
■ Какие технологии замените/добавите?
■ Какую архитектуру ETL предложите?
■ Какие метрики мониторинга ETL вы бы внедрили?
■ Где будут храниться входные и выходные данные?